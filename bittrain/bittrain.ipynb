{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import concurrent.futures\n",
    "import re\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a new chroma instance\n",
    "# add some documents and persist to directory\n",
    "# query using filter on meta data\n",
    "\n",
    "# create a new chroma instance\n",
    "embeddings = OpenAIEmbeddings( openai_api_key=\"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\")\n",
    "db = Chroma(persist_directory=\"abccosniedb\", embedding_function=embeddings, collection_name=\"customerservice\", collection_metadata = {\"hnsw:space\": \"cosine\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in db._client.list_collections():\n",
    "  ids = collection.get()['ids']\n",
    "  print('REMOVE %s document(s) from %s collection' % (str(len(ids)), collection.name))\n",
    "ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'bc400f9a-2f17-11ee-814f-da4f0344683e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['bc400f9a-2f17-11ee-814f-da4f0344683e'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['How can I send money using this feature?\\nThe process of sending money has not changed. Instead of using the IBAN, you can use the mobile number of the beneficiary. Your beneficiary must have at least one registered, valid bank account in BenefitPay or is an ila customer who have set the 창\\x80\\x9cReceive Money Using Mobile Number창\\x80\\x9d option on for you to use this feature to send money.\\nCan I receive payments from an ila customer using my mobile number?\\nYes, as an ila customer, you will be able to receive funds from ila customer using your mobile number. For this, you must have set the 창\\x80\\x9cReceive Money Using Mobile Number창\\x80\\x9d option ON or your ila account must be registered with BenefitPay.\\nCan I receive payments from another bank customer using my mobile number?\\nYes, as an ila customer, you will be able to receive funds from another bank customer using your mobile number. For this, you must have registered your ila account in BenefitPay.'],\n",
       " 'metadatas': [{'source': 'https:||ilabank.com|faqs.txt',\n",
       "   'country': 'bahrain',\n",
       "   'businessline': 'ila',\n",
       "   'tags': 'https:, ilabank.com, faqs'}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(ids='bc400f9a-2f17-11ee-814f-da4f0344683e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ \n",
    "    Document(page_content=\"This is document 1\", metadata={\"category\": \"A\"}),\n",
    "    # ... more documents\n",
    "]\n",
    "\n",
    "# Now you can extract page_content \n",
    "texts = [doc.page_content for doc in documents]\n",
    "metadatas = [doc.metadata for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now call add_documents with a list of documents\n",
    "db.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some documents and persist to directory\n",
    "# document 1\n",
    "doc1 = {'id': 1, 'text': 'This is document 1', 'meta': {'category': 'A'}}\n",
    "\n",
    "db.add_documents(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a new chroma instance\n",
    "# add some documents and persist to directory\n",
    "# query using filter on meta data\n",
    "# update using ID\n",
    "# create a new chroma instance\n",
    "embeddings = OpenAIEmbeddings()\n",
    "my_db = Chroma(persist_directory=\"my_db_dest\", embedding_function=embeddings)\n",
    "\n",
    "# add some documents and persist to directory\n",
    "# document 1\n",
    "doc1 = {'id': 1, 'text': 'This is document 1', 'meta': {'category': 'A'}}\n",
    "my_db.add_document(doc1)\n",
    "\n",
    "# document 2\n",
    "doc2 = {'id': 2, 'text': 'This is document 2', 'meta': {'category': 'B'}}\n",
    "my_db.add_document(doc2)\n",
    "\n",
    "# persist to directory\n",
    "my_db.persist()\n",
    "\n",
    "# query using filter on meta data\n",
    "category_filter = {'meta.category': 'A'}\n",
    "results = my_db.query(filter=category_filter)\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# update using ID\n",
    "doc_id = 1\n",
    "new_text = 'This is the updated document 1'\n",
    "my_db.update_document(doc_id, {'text': new_text})\n",
    "\n",
    "# delete using ID\n",
    "doc_id = 2\n",
    "my_db.delete_document(doc_id)\n",
    "# delete using ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onDemandload(start_url, urltag, main_content_class):\n",
    "    max_depth = 0  # Set the desired maximum depth for scraping\n",
    "    main_content_class = \"PageContent\"  # this is for abc bank \n",
    "    embeddings = OpenAIEmbeddings( openai_api_key=\"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\")\n",
    "    db = Chroma(persist_directory=\"abctitledb\", embedding_function=embeddings, collection_name=\"customerservice\", collection_metadata = {\"hnsw:space\": \"cosine\"})\n",
    "    scrape_links(start_url, max_depth, urltag, main_content_class )\n",
    "    \n",
    "    filename = start_url.replace('/', '|') + '.txt'\n",
    "    if os.path.exists(filename):\n",
    "        country, business_unit = extract_country_and_business_unit(filename)\n",
    "        base_url = \"https:||wwww.bank-abc.com\"\n",
    "                \n",
    "        end_extension = \".aspx\"\n",
    "\n",
    "        # Remove base URL\n",
    "        token_string = filename.replace(base_url, \"\")\n",
    "        token_string = token_string.replace(end_extension, \"\")\n",
    "        end_extension = \".txt\"\n",
    "        token_string = token_string.replace(end_extension, \"\")\n",
    "\n",
    "        # Split into tokens\n",
    "        tokens = token_string.split(\"|\")\n",
    "\n",
    "        # Remove empty strings from tokens (as the string starts with '/')\n",
    "        tokens = [token for token in tokens if token]\n",
    "        \n",
    "        tokens_string = ', '.join(tokens)\n",
    "        loader = TextLoader(filename)\n",
    "        documents = loader.load()\n",
    "        documents[0].metadata['country'] = country\n",
    "        documents[0].metadata['businessline'] = business_unit\n",
    "        documents[0].metadata['tags'] = tokens_string\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        db.add_documents(docs)\n",
    "    else:\n",
    "        print(f\"File not found: {filename}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import concurrent.futures\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import shutil\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from collections import deque\n",
    "import re\n",
    "from collections import namedtuple\n",
    "import openai\n",
    "\n",
    "openai.api_key = ''\n",
    "openai_api_key = ''\n",
    "\n",
    "# Take an URL as input and scrape the page for links and text\n",
    "\n",
    "\n",
    "\n",
    "def save_visited_urls(visited, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for url in visited:\n",
    "            f.write(url + '\\n')\n",
    "\n",
    "def load_visited_urls(filename):\n",
    "    visited = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                visited.add(line.strip())\n",
    "    return visited\n",
    "\n",
    "def get_links_and_text(url, urltag, main_content_class=None):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    if main_content_class:\n",
    "        main_content_tags = soup.find_all(class_=main_content_class)\n",
    "    else:\n",
    "        main_content_tags = []  # Set main_content_tags to an empty list if no class is provided\n",
    "    \n",
    "    extracted_text = \"\"\n",
    "    texts = \"\"  # Initialize texts with an empty string before the loop\n",
    "\n",
    "    if main_content_tags:\n",
    "        for tag in main_content_tags:\n",
    "            texts += tag.get_text() + \"\\n\"  # Concatenate all texts\n",
    "    else:\n",
    "        texts = soup.get_text(\"\\n\", strip=True)  # Get the entire page's text if main_content_tags is not found\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            # Construct absolute URL if necessary\n",
    "            absolute_url = urljoin(url, href)\n",
    "            if urltag in absolute_url:\n",
    "                links.append(absolute_url)\n",
    "    #print(\"links are : \" + str(links))\n",
    "    return links, texts\n",
    "\n",
    "def process_url(url, urltag, main_content_class=None):\n",
    "    print(f'Scraping {url}...')\n",
    "    # If the URL is a PDF, download it directly\n",
    "    if url.endswith('.pdf'):\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        filename = url.replace('/', '-') + '.pdf'  # Use the last part of the URL as the filename\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return []\n",
    "    else:\n",
    "        # Otherwise, continue with the normal scraping process\n",
    "        links, text = get_links_and_text(url, urltag, main_content_class)\n",
    "        # Save the text content in a separate file\n",
    "        filename = url.replace('/', '|') + '.txt'\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(text)\n",
    "        return links\n",
    "\n",
    "def scrape_links(start_url, max_depth, urltag, main_content_class=None):\n",
    "    visited = load_visited_urls('processed_urls.txt')\n",
    "    urls_queue = deque([(start_url, 0)])  # Start with depth 0\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        while urls_queue:\n",
    "            url, depth = urls_queue.popleft()\n",
    "            if url not in visited and depth <= max_depth:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    # Process the URL and get the links\n",
    "                    links = process_url(url, urltag, main_content_class)\n",
    "                    # Add new links to the queue with increased depth\n",
    "                    urls_queue.extend((link, depth + 1) for link in links)\n",
    "                    # Save the URL to the file immediately after it's processed\n",
    "                    with open('urls.txt', 'a') as f:\n",
    "                        f.write(url + '\\n')\n",
    "                    # Remember to respect the website's policy on web scraping\n",
    "                    time.sleep(7)\n",
    "                except Exception as e:\n",
    "                    print(f'Error scraping {url}: {e}')\n",
    "\n",
    "            # Process URLs one by one\n",
    "            if url not in visited and depth <= max_depth:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    # Process the URL\n",
    "                    process_url(url, urltag, main_content_class)\n",
    "                    # Update the processed URLs file\n",
    "                    with open('processed_urls.txt', 'a') as pf:\n",
    "                        pf.write(url + '\\n')\n",
    "                except Exception as e:\n",
    "                    print(f'Error processing {url}: {e}')\n",
    "                    \n",
    "def move_to_processed_folder(file_path):\n",
    "    # Create the 'processed' folder if it doesn't exist\n",
    "    if not os.path.exists(\"processed\"):\n",
    "        os.mkdir(\"processed\")\n",
    "\n",
    "    # Move the processed file to the 'processed' folder\n",
    "    destination = os.path.join(\"processed\", os.path.basename(file_path))\n",
    "    shutil.move(file_path, destination)\n",
    "    print(f\"Moved processed file to {destination}\")\n",
    "\n",
    "def extract_country_and_business_unit(filename):\n",
    "    countries = {\"Europe\", \"Bahrain\", \"IB\", \"Jordan\", \"Egypt\", \"Algeria\", \"Tunis\", \"Brazil\",\"United Arab Emirates\"}\n",
    "\n",
    "    business_units = {\"Wholesale\", \"Retail\", \"ila\",\"Islamic\"}\n",
    "\n",
    "    # Default values\n",
    "    default_country = \"bahrain\"\n",
    "    default_business_unit = \"general\"\n",
    "\n",
    "    # Convert filename to lowercase for case-insensitive search\n",
    "    lower_filename = filename.lower()\n",
    "\n",
    "    # Search for the country in the filename\n",
    "    for country in countries:\n",
    "        if country.lower() in lower_filename:\n",
    "            identified_country = country\n",
    "            break\n",
    "    else:\n",
    "        identified_country = default_country\n",
    "\n",
    "    # Search for the business unit in the filename\n",
    "    for business_unit in business_units:\n",
    "        if business_unit.lower() in lower_filename:\n",
    "            identified_business_unit = business_unit\n",
    "            break\n",
    "    else:\n",
    "        identified_business_unit = default_business_unit\n",
    "\n",
    "    return identified_country, identified_business_unit\n",
    "\n",
    "def getTitleTemplate(page_content, country):\n",
    "        #using the data from the digital twin create a formatted template\n",
    "        \n",
    "        DEFAULT_TITLE_NODE_TEMPLATE = f\"\\nContext: {page_content}, Country is {country}.\\n\\nNow, Give a title that summarizes all of the unique entities, titles or themes found in the context. \\nTitle: \"\n",
    "                \n",
    "        # combine everything\n",
    "        \n",
    "        formatted_template = DEFAULT_TITLE_NODE_TEMPLATE\n",
    "        print(\"formatted template is : \" + formatted_template)\n",
    "        #formatted_template = formatted_persona_objective\n",
    "\n",
    "        return formatted_template\n",
    "    \n",
    "def GetDocumentTitle(TitleTemplate):\n",
    "        \n",
    "\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                response = openai.Completion.create(engine=\"text-davinci-003\", prompt=TitleTemplate, max_tokens=100)\n",
    "                title = response.choices[0].text.strip()\n",
    "                time.sleep(2)\n",
    "                print(\"Title is : \" + title)\n",
    "                return title\n",
    "            except openai.error.APIConnectionError:  # Catch the specific exception for a timeout\n",
    "                print(f\"Timeout {i+1}. Retrying...\")\n",
    "                time.sleep((2 ** i))  # Exponential backoff: wait for 1, 2, 4, 8... seconds\n",
    "            return None  # If we're out of retries, return None (or you could raise an exception)\n",
    "\n",
    "def parse_documents(docs):\n",
    "    result = []\n",
    "    for document in docs:\n",
    "        # Extract metadata with error handling\n",
    "        source = document.metadata.get('source', '')\n",
    "        country = document.metadata.get('country', '')\n",
    "        businessline = document.metadata.get('businessline', '')\n",
    "        tags = document.metadata.get('tags', '')\n",
    "\n",
    "        page_content = document.page_content\n",
    "        \n",
    "        titleTemplate = getTitleTemplate(page_content, country)\n",
    "        \n",
    "        print(\"titleTemplate is : \" + titleTemplate)\n",
    "        \n",
    "        newPageContent =  \"Page Title : \" + GetDocumentTitle(titleTemplate) + \"\\n Page Content : \" + page_content\n",
    "        \n",
    "        \n",
    "        # Create new Document instance and append to result\n",
    "        result.append(\n",
    "            Document(\n",
    "                page_content=newPageContent,\n",
    "                metadata={\n",
    "                    'source': source,\n",
    "                    'country': country,\n",
    "                    'businessline': businessline,\n",
    "                    'tags': tags\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return result\n",
    "\n",
    "def onDemandload(start_url, urltag, main_content_class):\n",
    "    max_depth = 0  # Set the desired maximum depth for scraping\n",
    "    main_content_class = \"PageContent\"  # this is for abc bank \n",
    "    embeddings = OpenAIEmbeddings( openai_api_key=\"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\")\n",
    "    db = Chroma(persist_directory=\"abctitledb\", embedding_function=embeddings, collection_name=\"customerservice\", collection_metadata = {\"hnsw:space\": \"cosine\"})\n",
    "    scrape_links(start_url, max_depth, urltag, main_content_class )\n",
    "    \n",
    "    filename = start_url.replace('/', '|') + '.txt'\n",
    "    if os.path.exists(filename):\n",
    "        country, business_unit = extract_country_and_business_unit(filename)\n",
    "        base_url = \"https:||wwww.bank-abc.com\"\n",
    "                \n",
    "        end_extension = \".aspx\"\n",
    "\n",
    "        # Remove base URL\n",
    "        token_string = filename.replace(base_url, \"\")\n",
    "        token_string = token_string.replace(end_extension, \"\")\n",
    "        end_extension = \".txt\"\n",
    "        token_string = token_string.replace(end_extension, \"\")\n",
    "\n",
    "        # Split into tokens\n",
    "        tokens = token_string.split(\"|\")\n",
    "\n",
    "        # Remove empty strings from tokens (as the string starts with '/')\n",
    "        tokens = [token for token in tokens if token]\n",
    "        \n",
    "        tokens_string = ', '.join(tokens)\n",
    "        loader = TextLoader(filename)\n",
    "        documents = loader.load()\n",
    "        documents[0].metadata['country'] = country\n",
    "        documents[0].metadata['businessline'] = business_unit\n",
    "        documents[0].metadata['tags'] = tokens_string\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        db.add_documents(docs)\n",
    "    else:\n",
    "        print(f\"File not found: {filename}\")\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://vedabase.io/en/library/bg/\"  # Replace with the desired starting URL\n",
    "    max_depth = 10  # Set the desired maximum depth for scraping\n",
    "    main_content_class = \"PageContent\"  # this is for abc bank\n",
    "    main_content_class = \"r r-title r-chapter\"\n",
    "    #main_content_class = \"no-js\"  # this is for times of india\n",
    "    urltag = \"vedabase.io/en/library/bg\"  # Replace with the desired URL tag to filter URLs\n",
    "    \n",
    "    #uncomment the below line to start scraping\n",
    "    #scrape_links(start_url, max_depth, urltag, main_content_class )\n",
    "    \n",
    "    onDemandload(\"https://docs.trychroma.com/\", \"docs.trychroma.com\", \"no-js\")\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "    #instantiate a db instance with the embedding function\n",
    "    embeddings = OpenAIEmbeddings( openai_api_key=\"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\")\n",
    "    db = Chroma(persist_directory=\"abctitledb\", embedding_function=embeddings, collection_name=\"customerservice\", collection_metadata = {\"hnsw:space\": \"cosine\"})\n",
    "    \n",
    "    #delete any content that already exists\n",
    "    \n",
    "    \n",
    "    #for each file in url.text\n",
    "    # extract the country and business unit\n",
    "    \n",
    "    #db.add_documents(docs)\n",
    "    with open(\"urls.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            file_path = line.strip()\n",
    "            filename = file_path.replace('/', '|') + '.txt'\n",
    "            if os.path.exists(filename):\n",
    "                 # extract the country and business unit\n",
    "                country, business_unit = extract_country_and_business_unit(filename)\n",
    "                \n",
    "                \n",
    "                \n",
    "                base_url = \"https:||wwww.bank-abc.com\"\n",
    "                \n",
    "                end_extension = \".aspx\"\n",
    "\n",
    "                # Remove base URL\n",
    "                token_string = filename.replace(base_url, \"\")\n",
    "                token_string = token_string.replace(end_extension, \"\")\n",
    "                end_extension = \".txt\"\n",
    "                token_string = token_string.replace(end_extension, \"\")\n",
    "\n",
    "                # Split into tokens\n",
    "                tokens = token_string.split(\"|\")\n",
    "\n",
    "                # Remove empty strings from tokens (as the string starts with '/')\n",
    "                tokens = [token for token in tokens if token]\n",
    "                \n",
    "                tokens_string = ', '.join(tokens)\n",
    "                \n",
    "                \n",
    "                # generate chunk and store the chunk in the database\n",
    "                #Create chunk for question generation\n",
    "                #sourcefile = url.replace('/', '_') + '.txt'\n",
    "                loader = TextLoader(filename)\n",
    "                documents = loader.load()\n",
    "                documents[0].metadata['country'] = country\n",
    "                documents[0].metadata['businessline'] = business_unit\n",
    "                documents[0].metadata['tags'] = tokens_string\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "                docs = text_splitter.split_documents(documents)\n",
    "                result = parse_documents(docs)\n",
    "\n",
    "                print(\"results are : \" + str(result))\n",
    "                \n",
    "                #print(\"docs are : \" + str(docs))\n",
    "                \n",
    "                \n",
    "                #print(\"docs are : \" + str(docs))\n",
    "                \n",
    "                \n",
    "                #for doc in docs:\n",
    "                #    doc.page_content = tokens_string + '\\n\\n' + doc.page_content\n",
    "                #print(\"docs are : \" + str(docs))\n",
    "                #exit()\n",
    "                #docs.extend(text_splitter.split_documents(documents))\n",
    "                db.add_documents(result)\n",
    "                \n",
    "                \n",
    "                #move_to_processed_folder(file_path)\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
