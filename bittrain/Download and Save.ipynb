{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import concurrent.futures\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an URL as input and scrape the page for links and text\n",
    "\n",
    "\n",
    "\n",
    "def save_visited_urls(visited, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for url in visited:\n",
    "            f.write(url + '\\n')\n",
    "\n",
    "def load_visited_urls(filename):\n",
    "    visited = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                visited.add(line.strip())\n",
    "    return visited\n",
    "\n",
    "def get_links_and_text(url, main_content_class=None):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    if main_content_class:\n",
    "        main_content_tags = soup.find_all(class_=main_content_class)\n",
    "    else:\n",
    "        main_content_tags = []  # Set main_content_tags to an empty list if no class is provided\n",
    "    \n",
    "    extracted_text = \"\"\n",
    "    texts = \"\"  # Initialize texts with an empty string before the loop\n",
    "\n",
    "    if main_content_tags:\n",
    "        for tag in main_content_tags:\n",
    "            texts += tag.get_text() + \"\\n\"  # Concatenate all texts\n",
    "    else:\n",
    "        texts = soup.get_text(\".\", strip=True)  # Get the entire page's text if main_content_tags is not found\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            # Construct absolute URL if necessary\n",
    "            absolute_url = urljoin(url, href)\n",
    "            if 'bank-abc' in absolute_url:\n",
    "                links.append(absolute_url)\n",
    "\n",
    "    return links, texts\n",
    "\n",
    "def process_url(url, main_content_class=None):\n",
    "    print(f'Scraping {url}...')\n",
    "    # If the URL is a PDF, download it directly\n",
    "    if url.endswith('.pdf'):\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        filename = url.replace('/', '-') + '.pdf'  # Use the last part of the URL as the filename\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return []\n",
    "    else:\n",
    "        # Otherwise, continue with the normal scraping process\n",
    "        links, text = get_links_and_text(url, main_content_class)\n",
    "        # Save the text content in a separate file\n",
    "        filename = url.replace('/', '|') + '.txt'\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(text)\n",
    "        return links\n",
    "\n",
    "def scrape_links(start_url, max_depth, main_content_class=None):\n",
    "    # Load previously processed URLs\n",
    "    visited = load_visited_urls('processed_urls.txt')\n",
    "\n",
    "    urls = {start_url: 0}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        while urls:\n",
    "            url, depth = urls.popitem()\n",
    "            if url not in visited and depth <= max_depth:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    # Pass the main content class name as an argument\n",
    "                    links = process_url(url, main_content_class)\n",
    "                    urls.update({link: depth + 1 for link in links})\n",
    "                    # Save the URL to the file immediately after it's processed\n",
    "                    with open('urls.txt', 'a') as f:\n",
    "                        f.write(url + '\\n')\n",
    "                    # Remember to respect the website's policy on web scraping\n",
    "                    time.sleep(5)\n",
    "                except Exception as e:\n",
    "                    print(f'Error scraping {url}: {e}')\n",
    "\n",
    "    # Process URLs one by one\n",
    "    with open('urls.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            url = line.strip()\n",
    "            if url not in visited:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    # Pass the main content class name as an argument\n",
    "                    process_url(url, main_content_class)\n",
    "                    # Update the processed URLs file\n",
    "                    with open('processed_urls.txt', 'a') as pf:\n",
    "                        pf.write(url + '\\n')\n",
    "                except Exception as e:\n",
    "                    print(f'Error processing {url}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "start_url = \"https://www.bank-abc.com/En/Pages/default.aspx\"  # Replace with the desired starting URL\n",
    "max_depth = 1  # Set the desired maximum depth for scraping\n",
    "#main_content_class = \"MainContent\"  # Replace with the specific class name for the main content (Optional)\n",
    "main_content_class = \"PageContent\"  # Replace with the specific class name for the main content (Optional)\n",
    "scrape_links(start_url, max_depth,main_content_class )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Each file\n",
    "sourcefile = \"https:||www.bank-abc.com|world|Algeria|En|Products|RetailBanking|Pages|default.aspx.txt\"\n",
    "\n",
    "#Load the file\n",
    "\n",
    "\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Instantiate OpenAI language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_and_business_unit(filename):\n",
    "    countries = {\"Europe\", \"Jordan\", \"Egypt\", \"Algeria\", \"Tunisia\", \"Brazil\", \"Bank ABC Islamic\",\n",
    "                 \"ila Bank\", \"ABC Blom Egypt\"}\n",
    "\n",
    "    business_units = {\"Wholesale\", \"Retail\", \"Transactional\", \"Corporate\", \"Treasury\", \"Private\", \"Islamic\", \"ila\", }\n",
    "\n",
    "    # Default values\n",
    "    default_country = \"Bahrain\"\n",
    "    default_business_unit = \"General\"\n",
    "\n",
    "    # Convert filename to lowercase for case-insensitive search\n",
    "    lower_filename = filename.lower()\n",
    "\n",
    "    # Search for the country in the filename\n",
    "    for country in countries:\n",
    "        if country.lower() in lower_filename:\n",
    "            identified_country = country\n",
    "            break\n",
    "    else:\n",
    "        identified_country = default_country\n",
    "\n",
    "    # Search for the business unit in the filename\n",
    "    for business_unit in business_units:\n",
    "        if business_unit.lower() in lower_filename:\n",
    "            identified_business_unit = business_unit\n",
    "            break\n",
    "    else:\n",
    "        identified_business_unit = default_business_unit\n",
    "\n",
    "    return identified_country, identified_business_unit\n",
    "\n",
    "\n",
    "\n",
    "country, business_unit = extract_country_and_business_unit(sourcefile)\n",
    "\n",
    "print(\"Identified Country:\", country)\n",
    "print(\"Identified Business Unit:\", business_unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#part 2: generate response to the query\n",
    "\n",
    "#load the chroma DB into memory and create a retriever\n",
    "#Get the relevant context documents using similarity search with score\n",
    "#get a standalone question\n",
    "# generate response using the context documents\n",
    "#return the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create chunk for question generation\n",
    "#sourcefile = url.replace('/', '_') + '.txt'\n",
    "loader = TextLoader(sourcefile)\n",
    "documents = loader.load()\n",
    "documents[0].metadata['country'] = country\n",
    "documents[0].metadata['businessline'] = business_unit\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "#docs.extend(text_splitter.split_documents(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the chunks into chroma DB, along with Metadata dictionary using openai embeddings\n",
    "# Split the documents into chunks\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=2)\n",
    "#texts = text_splitter.split_documents(documents)\n",
    "\n",
    "#####here is where you append the metadata before saving into the database\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "persist_directory = 'database'\n",
    "# Create the vector store (Chroma) and persist it\n",
    "db = Chroma.from_documents(docs, embedding=embeddings,collection_name=\"Govind\",persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama_index --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=\"bankabcdb\", embedding_function=embeddings, collection_name=\"customerservice\")\n",
    "db.delete_collection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# load from disk\n",
    "db3 = Chroma(persist_directory=\"database\", embedding_function=embeddings)\n",
    "\n",
    "db3 = Chroma.from_documents(docs, embedding=embeddings,collection_name=\"Govind\",persist_directory=persist_directory)\n",
    "\n",
    "\n",
    "db4 = Chroma(persist_directory=\"database\", embedding_function=embeddings, collection_name=\"Govind\")\n",
    "db4.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 : Crawl the webpages and store the text in a file\n",
    "\n",
    "\n",
    "\n",
    "#store the metadata in a dictionary\n",
    "#Split ther file into chunks\n",
    "#store the chunks into chroma DB, along with Metadata dictionary using openai embeddings\n",
    "\n",
    "\n",
    "#part 2: generate response to the query\n",
    "\n",
    "#load the chroma DB into memory and create a retriever\n",
    "#Get the relevant context documents using similarity search with score\n",
    "#get a standalone question\n",
    "# generate response using the context documents\n",
    "#return the response\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "\n",
    "retriever\n",
    "# add documents to chroma DB\n",
    "\n",
    "#db.similarity_search_with_relevance_scores()\n",
    "#you can add and manipulate at collection level, problem is in retrieval, it does not filter based on the metadata \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query it\n",
    "query = \"what are the products offerred for retail banking?\"\n",
    "#docs = db.similarity_search(query,filter={\"country\":\"Egypt\"})\n",
    "\n",
    "docs = db5.similarity_search_with_score(query,filter={\"country\":\"Jordan\"}, k=4)\n",
    "\n",
    "\n",
    "\n",
    "# print results\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, Embeddings\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"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\",\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db._collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"Shukla\",embedding_function=openai_ef,metadata={\"hnsw:space\": \"cosine\"} )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek() # returns a list of the first 10 items in the collection\n",
    "collection.count() # returns the number of items in the collection\n",
    "collection.modify(name=\"new_name\") # Rename the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\"lorem ipsum...\", \"doc2\", \"doc3\"],\n",
    "    metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, {\"chapter\": \"3\", \"verse\": \"5\"}, {\"chapter\": \"29\", \"verse\": \"11\"}],\n",
    "    ids=[\"id4\", \"id5\", \"id6\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Handle the case where the user doesn't provide ids on the Collection\n",
    "import uuid\n",
    "\n",
    "\n",
    "texts = [doc.page_content for doc in documents]\n",
    "metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "ids = [str(uuid.uuid1()) for _ in texts]\n",
    "collection.add(\n",
    "        metadatas=metadatas, documents=texts, ids=ids\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collection.query(\n",
    "    query_texts=[\"doc3\"],\n",
    "    n_results=2,\n",
    "    where={\"chapter\": \"3\"},\n",
    "    include=[\"documents\"]\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(collection.peek(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_passage(query, db):\n",
    "  passage = db.query(query_texts=[query], n_results=2)['documents']\n",
    "  return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform embedding search\n",
    "passage = get_relevant_passage(\"i would like to open a account\", collection)\n",
    "passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = collection.get(\n",
    "    ids=[\"id4\", \"id2\", \"b06589e4-2656-11ee-90e1-da4f0344683e\"]\n",
    ")\n",
    "\n",
    "pd.DataFrame(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db5 = Chroma(persist_directory=\"abcdb\", embedding_function=embeddings, collection_name=\"customerservice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "# Assuming this is the structure of your Document\n",
    "Document = namedtuple('Document', ['page_content', 'metadata'])\n",
    "\n",
    "def parse_documents(documents):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        \n",
    "        \n",
    "        \n",
    "        result.append(\n",
    "            {\n",
    "            'pagecontent': document.page_content,\n",
    "            'source': document.metadata['source'],\n",
    "            'country': document.metadata['country'],\n",
    "            'businessline': document.metadata['businessline'],\n",
    "            'tags': document.metadata['tags']\n",
    "            })    \n",
    "    return result\n",
    "        \n",
    "\n",
    "documents = [\n",
    "    Document(page_content='Page Content1. \\u200bArab Leasing Corporation (ALC)\\u200b: 41%\\n\\xa0\\u200b\\u200bChief Executive Officer: Mr. Abdelhakim Djebarni\\nShare CapitalThe Share Capital of ALC is\\xa0 3 500 023 744 DA.', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='2. \\u200bBank Deposit Guarantee Fund (C.G.D.B): 3.70%', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='3. \\u200bInterbank Pre-Compensation Center (CPI): 0.66%', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='Shareholders of Bank ABC Algeria\\u200b\\u200b\\u200b\\n\\u200b%\\nArab Banking Corporation (BSC)\\n87.66%\\n\\u200bThe Arab Investment Company (TAIC)\\n\\u200b4.18%\\nSociété Financière Internationale (IFC)\\n\\u200b2.32%', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content=\"\\u200bCompagnie Algérienne d'assurances et de réassurance (CAAR)\\n\\u200b2.09%\\n\\u200bSPMC\\n\\u200b1.34%\\n\\u200bModern Ceramic\\n\\u200b0.92%\\n\\u200bHolding Mestour\\n\\u200b0.92%\\n\\u200bConsorts OTHMANI\\n\\u200b0.38%\\nTraplast Sanitaire\\n0.19%\\u200b\\n\\u200bTotal\\n\\u200b100%\\u200b\", metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'})\n",
    "]\n",
    "\n",
    "result = parse_documents(documents)\n",
    "\n",
    "print(result)\n",
    "\n",
    "#for r in result:\n",
    "#    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import namedtuple\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "openai_api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleTemplate(page_content, country):\n",
    "        #using the data from the digital twin create a formatted template\n",
    "        \n",
    "        DEFAULT_TITLE_NODE_TEMPLATE = f\"\\nContext: {page_content}, Country is {country}.\\n\\nNow, Give a title that summarizes all of the unique entities, titles or themes found in the context, in case there is not enough information, return blank. \\nTitle: \"\n",
    "                \n",
    "        # combine everything\n",
    "        \n",
    "        formatted_template = DEFAULT_TITLE_NODE_TEMPLATE\n",
    "        print(\"formatted template is : \" + formatted_template)\n",
    "        #formatted_template = formatted_persona_objective\n",
    "\n",
    "        return formatted_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDocumentTitle(TitleTemplate):\n",
    "        \n",
    "\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                response = openai.Completion.create(engine=\"text-davinci-003\", prompt=TitleTemplate, max_tokens=100)\n",
    "                stage = response.choices[0].text.strip()\n",
    "                print(\"stage is : \" + stage)\n",
    "                return stage\n",
    "            except openai.error.APIConnectionError:  # Catch the specific exception for a timeout\n",
    "                print(f\"Timeout {i+1}. Retrying...\")\n",
    "                time.sleep((2 ** i))  # Exponential backoff: wait for 1, 2, 4, 8... seconds\n",
    "            return None  # If we're out of retries, return None (or you could raise an exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming this is the structure of your Document\n",
    "Document = namedtuple('Document', ['page_content', 'metadata'])\n",
    "\n",
    "def parse_documents(docs):\n",
    "    result = []\n",
    "    for document in docs:\n",
    "        # Extract metadata with error handling\n",
    "        source = document.metadata.get('source', '')\n",
    "        country = document.metadata.get('country', '')\n",
    "        businessline = document.metadata.get('businessline', '')\n",
    "        tags = document.metadata.get('tags', '')\n",
    "\n",
    "        page_content = document.page_content\n",
    "        \n",
    "        titleTemplate = getTitleTemplate(page_content, country)\n",
    "        \n",
    "        print(\"titleTemplate is : \" + titleTemplate)\n",
    "        \n",
    "        newPageContent =  \"Page Title : \" + GetDocumentTitle(titleTemplate) + \"\\n Page Content : \" + page_content\n",
    "        \n",
    "        \n",
    "        # Create new Document instance and append to result\n",
    "        result.append(\n",
    "            Document(\n",
    "                page_content=newPageContent,\n",
    "                metadata={\n",
    "                    'source': source,\n",
    "                    'country': country,\n",
    "                    'businessline': businessline,\n",
    "                    'tags': tags\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return result\n",
    "        \n",
    "\n",
    "docs = [\n",
    "    Document(page_content='Page Content1. \\u200bArab Leasing Corporation (ALC)\\u200b: 41%\\n\\xa0\\u200b\\u200bChief Executive Officer: Mr. Abdelhakim Djebarni\\nShare CapitalThe Share Capital of ALC is\\xa0 3 500 023 744 DA.', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='2. \\u200bBank Deposit Guarantee Fund (C.G.D.B): 3.70%', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='3. \\u200bInterbank Pre-Compensation Center (CPI): 0.66%', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='Shareholders of Bank ABC Algeria\\u200b\\u200b\\u200b\\n\\u200b%\\nArab Banking Corporation (BSC)\\n87.66%\\n\\u200bThe Arab Investment Company (TAIC)\\n\\u200b4.18%\\nSociété Financière Internationale (IFC)\\n\\u200b2.32%', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content=\"\\u200bCompagnie Algérienne d'assurances et de réassurance (CAAR)\\n\\u200b2.09%\\n\\u200bSPMC\\n\\u200b1.34%\\n\\u200bModern Ceramic\\n\\u200b0.92%\\n\\u200bHolding Mestour\\n\\u200b0.92%\\n\\u200bConsorts OTHMANI\\n\\u200b0.38%\\nTraplast Sanitaire\\n0.19%\\u200b\\n\\u200bTotal\\n\\u200b100%\\u200b\", metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'}), Document(page_content='\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b', metadata={'source': 'https:||www.bank-abc.com|world|Algeria|En|AboutABCNew|Financial|Pages|Investements.aspx.txt', 'country': 'Algeria', 'businessline': 'general', 'tags': 'https:, www.bank-abc.com, world, Algeria, En, AboutABCNew, Financial, Pages, Investements'})\n",
    "]\n",
    "\n",
    "result = parse_documents(docs)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor,\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    MetadataFeatureExtractor,\n",
    ")\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "\n",
    "class CustomExtractor(MetadataFeatureExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": node.metadata[\"document_title\"]\n",
    "                + \"\\n\"\n",
    "                + node.metadata[\"excerpt_keywords\"]\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n",
    "\n",
    "\n",
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        TitleExtractor(nodes=5),\n",
    "        QuestionsAnsweredExtractor(questions=3),\n",
    "        # SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
    "        # KeywordExtractor(keywords=10),\n",
    "        # CustomExtractor()\n",
    "    ],\n",
    ")\n",
    "\n",
    "node_parser = SimpleNodeParser(\n",
    "    text_splitter=text_splitter,\n",
    "    metadata_extractor=metadata_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "payload = json.dumps({\n",
    "  \"q\": \"What products does bank abc offer?\"\n",
    "})\n",
    "headers = {\n",
    "  'X-API-KEY': '7e5445663d8bf0f45167042f32eaeea46426395d',\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "conn.request(\"POST\", \"/search\", payload, headers)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "payload = json.dumps({\n",
    "    \"q\": \"whatg products does bank abc offers\"\n",
    "})\n",
    "headers = {\n",
    "    'X-API-KEY': '7e5445663d8bf0f45167042f32eaeea46426395d',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "conn.request(\"POST\", \"/search\", payload, headers)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "# Decode the response into a JSON string\n",
    "json_data = data.decode('utf-8')\n",
    "\n",
    "# Parse the JSON data\n",
    "parsed_data = json.loads(json_data)\n",
    "\n",
    "#print(json.dumps(parsed_data, indent=2))\n",
    "\n",
    "# Extract relevant information\n",
    "\n",
    "search_parameters = parsed_data[\"searchParameters\"]\n",
    "organic_results = parsed_data[\"organic\"]\n",
    "people_also_ask = parsed_data[\"peopleAlsoAsk\"]\n",
    "related_searches = parsed_data[\"relatedSearches\"]\n",
    "\n",
    "# Display extracted information\n",
    "print(\"Search Parameters:\")\n",
    "print(json.dumps(search_parameters, indent=2))\n",
    "\n",
    "print(\"\\nOrganic Results:\")\n",
    "for result in organic_results:\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "print(\"\\nPeople Also Ask:\")\n",
    "for question in people_also_ask:\n",
    "    print(json.dumps(question, indent=2))\n",
    "\n",
    "print(\"\\nRelated Searches:\")\n",
    "for query in related_searches:\n",
    "    print(json.dumps(query, indent=2))\n",
    "    \n",
    "if \"knowledgeGraph\" in parsed_data:\n",
    "    knowledge_graph = parsed_data[\"knowledgeGraph\"]\n",
    "    print(\"\\nKnowledge Graph:\")\n",
    "    print(json.dumps(knowledge_graph, indent=2))\n",
    "    \n",
    "    \n",
    "bank_abc_links = [result[\"link\"] for result in organic_results if \"www.bank-abc\" in result[\"link\"]]\n",
    "print(\"\\nBank ABC Links:\")\n",
    "print(bank_abc_links[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = json.loads(data)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(parsed_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondump =  json.dumps(parsed_data, indent=2)\n",
    "print(jsondump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(json.loads(jsondump)[\"searchParameters\"][\"q\"], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = json.loads(data.decode(\"utf-8\"))\n",
    "search_parameters = data[\"searchParameters\"]\n",
    "print(json.dumps(search_parameters, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jsondump.joinpath(\"google_search_results.json\").read_text()\n",
    "\n",
    "# Extract relevant information\n",
    "search_parameters = data[\"searchParameters\"]\n",
    "knowledge_graph = data.get(\"knowledgeGraph\", None)  # Use .get() to handle optional knowledge graph\n",
    "organic_results = data[\"organic\"]\n",
    "people_also_ask = data[\"peopleAlsoAsk\"]\n",
    "related_searches = data[\"relatedSearches\"]\n",
    "\n",
    "# Display extracted information\n",
    "print(\"Search Parameters:\")\n",
    "print(search_parameters)\n",
    "\n",
    "# Display Knowledge Graph if available\n",
    "if knowledge_graph:\n",
    "    print(\"\\nKnowledge Graph:\")\n",
    "    print(knowledge_graph)\n",
    "\n",
    "print(\"\\nOrganic Results:\")\n",
    "for result in organic_results:\n",
    "    print(result)\n",
    "\n",
    "print(\"\\nPeople Also Ask:\")\n",
    "for question in people_also_ask:\n",
    "    print(question)\n",
    "\n",
    "print(\"\\nRelated Searches:\")\n",
    "for query in related_searches:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://google.serper.dev/search\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"q\": \"Bank ABC Islamic\"\n",
    "})\n",
    "headers = {\n",
    "  'X-API-KEY': '7e5445663d8bf0f45167042f32eaeea46426395d',\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google serper API,\n",
    "\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"searchParameters\":{\"q\":\"Bank ABC Islamic\",\"type\":\"search\"},\"knowledgeGraph\":{\"title\":\"ABC Islamic Bank\",\"type\":\"Joint-stock company\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2MvyqbivoCE8F_mDKfW0N9MQyZIv5IHGYPKeGx14s1sRQg849ucUB-g&s=0\",\"description\":\"ABC Islamic Bank is a Joint Stock Company that provides investment banking services and asset management services in accordance with Islamic principles. It is a wholly owned subsidiary of Arab Banking Corporation. Its formerly known as ABC...\",\"descriptionSource\":\"Wikipedia\",\"descriptionLink\":\"https://en.wikipedia.org/wiki/ABC_Islamic_Bank\"},\"organic\":[{\"title\":\"Bank ABC Islamic\",\"link\":\"https://www.bank-abc.com/en/islamicbank/pages/default.aspx\",\"snippet\":\"Bank ABC provides a host of carefully formulated, Shari'a compliant products and services to cater to the demands of clients with an eye on the advantages ...\",\"sitelinks\":[{\"title\":\"Management\",\"link\":\"https://www.bank-abc.com/world/islamicbank/en/aboutabc/pages/management.aspx\"},{\"title\":\"Annual Reports\",\"link\":\"https://www.bank-abc.com/world/islamicbank/en/investment/pages/annualreports.aspx\"},{\"title\":\"Contact\",\"link\":\"https://www.bank-abc.com/world/islamicbank/en/pages/contact.aspx\"},{\"title\":\"About ABC Islamic\",\"link\":\"https://www.bank-abc.com/world/islamicbank/en/aboutabc/pages/default.aspx\"},{\"title\":\"Default.aspx\",\"link\":\"https://www.bank-abc.com/world/islamicbank/en/pages/default.aspx\"},{\"title\":\"Deals\",\"link\":\"https://www.bank-abc.com/world/islamicbank/en/deals/pages/default.aspx\"}],\"position\":1},{\"title\":\"ABC Islamic Bank - Wikipedia\",\"link\":\"https://en.wikipedia.org/wiki/ABC_Islamic_Bank\",\"snippet\":\"ABC Islamic Bank is a Joint Stock Company that provides investment banking services and asset management services in accordance with Islamic principles.\",\"position\":2},{\"title\":\"Bank ABC | LinkedIn\",\"link\":\"https://www.linkedin.com/company/bankabc\",\"snippet\":\"Bank ABC is a leading provider of Trade Finance, Treasury, Project & Structured Finance, Syndications, Corporate & Institutional Banking as well as Islamic ...\",\"position\":3},{\"title\":\"Bank ABC Islamic | LinkedIn\",\"link\":\"https://www.linkedin.com/showcase/bank-abc-islamic\",\"snippet\":\"Bank ABC Islamic (ABC Islamic Bank (E.C.)) is licensed as an Islamic wholesale bank by the Central Bank of Bahrain. Bank ABC Islamic is supported by the ABC ...\",\"position\":4},{\"title\":\"Bank ABC - Group Website\",\"link\":\"https://www.bank-abc.com/\",\"snippet\":\"Bank ABC is a leading provider of Trade Finance, Treasury, Project & Structured Finance, Syndications, Corporate & Institutional Banking as well as Islamic ...\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSB9KlyLOIl6WoKywMVQCBcL-KTrarH-QL51o_400sidaRqDm_2PUeZsDI&s\",\"position\":5},{\"title\":\"In Conversation with Hammad Hassan, Bank ABC Islamic | Global Finance Magazine\",\"link\":\"https://www.gfmag.com/media/expert-perspective/conversation-hammad-hassan-bank-abc-islamic\",\"snippet\":\"Hammad Hassan - Group Head of Islamic Banking & Managing Director, Bank ABC Islamic speaks with Global Finance's Founder and Editorial ...\",\"date\":\"Jul 3, 2023\",\"imageUrl\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSWewal80v1E8z2UAd3TlT4hAdHmzwUs-WC6qjz5noKOUFV3oiC9pWSpfI&s\",\"position\":6},{\"title\":\"Islamic Financial Services - Bank ABC\",\"link\":\"https://www.bank-abc.com/world/europe/En/Products/Pages/Islamic_Financial_Services.aspx\",\"snippet\":\"Bank ABC's Islamic Financial Services team in London offer a range of Shari'a compliant financial solutions to our clients in Europe and the MENAT region.\",\"position\":7}],\"peopleAlsoAsk\":[{\"question\":\"Is Bank ABC Islamic?\",\"snippet\":\"Bank ABC Islamic named Best Islamic Financial Institution in Bahrain in 2023. On\\nthe back of landmark transactions, continued digital transformation and\\nstrategic expansion, the Islamic Finance arm of Bank ABC excelled at Global\\nFinance's World's Best Islamic Financial Institutions Awards.\",\"title\":\"Bank ABC Islamic named Best Islamic Financial Institution in Bahrain in ...\",\"link\":\"https://www.bank-abc.com/world/IslamicBank/En/AboutABC/Media/Press/Pages/Best-Islamic-Financial-Institution-inBahrain2023.aspx\"},{\"question\":\"What is bank ABC?\",\"snippet\":\"Bank ABC (incorporated as Arab Banking Corporation B.S.C) is an international\\nbank headquartered in Manama, Kingdom of Bahrain. Our network spreads across\\nfive continents, covering countries in the Middle East, North Africa, Europe,\\nthe Americas and Asia.\",\"title\":\"Bank ABC - Group Website\",\"link\":\"https://www.bank-abc.com/\"},{\"question\":\"Who is the CEO of ABC Islamic Bank?\",\"snippet\":\"Mr. Saddek Omar El Kaber Chairman of the Board\\nMr. Sael Al Waary Group CEO of Bank ABC\",\"title\":\"Bank ABC Appoints Mr. Sael Al Waary as its Group Chief Executive Officer\",\"link\":\"https://www.bank-abc.com/En/AboutABC/Media/Press/Pages/Bank-ABC-Appoints-Sael-Al-Waary-as-its--Group-Chief-Executive-Officer.aspx\"},{\"question\":\"What is an Islamic bank account?\",\"snippet\":\"Islamic banking, also referred to as Islamic finance or Shariah-compliant\\nfinance, refers to financial activities that adhere to Shariah (Islamic law).\\nTwo fundamental principles of Islamic banking are the sharing of profit and loss\\nand the prohibition of the collection and payment of interest by lenders and\\ninvestors.\",\"title\":\"Islamic Banking and Finance Definition: History and Example\",\"link\":\"https://www.investopedia.com/terms/i/islamicbanking.asp\"}],\"relatedSearches\":[{\"query\":\"Bank ABC Annual Report\"},{\"query\":\"ABC International Bank PLC\"},{\"query\":\"Bank ABC Careers\"},{\"query\":\"ABC Bank wiki\"},{\"query\":\"Bank ABC linkedin\"},{\"query\":\"ABC Bank digital\"},{\"query\":\"Arab Banking Corporation subsidiaries\"},{\"query\":\"Bank ABC London\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"SERPER_API_KEY\"] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = GoogleSerperAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "self_ask_with_search.run(\"What is the hometown of the reigning men's U.S. Open champion?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "\n",
    "def get_bank_abc_links(question, baseurl):\n",
    "    conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "    payload = json.dumps({\n",
    "        \"q\": question\n",
    "    })\n",
    "    headers = {\n",
    "        'X-API-KEY': '7e5445663d8bf0f45167042f32eaeea46426395d',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    conn.request(\"POST\", \"/search\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "\n",
    "    # Decode the response into a JSON string\n",
    "    json_data = data.decode('utf-8')\n",
    "\n",
    "    # Parse the JSON data\n",
    "    parsed_data = json.loads(json_data)\n",
    "\n",
    "    # Extract relevant information\n",
    "    organic_results = parsed_data[\"organic\"]\n",
    "\n",
    "    # Extract Bank ABC Links\n",
    "    bank_abc_links = [result[\"link\"] for result in organic_results if baseurl in result[\"link\"]]\n",
    "\n",
    "    return bank_abc_links[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlfilter = \"www.bank-abc\"\n",
    "question = \"who is ceo of bank abc?\"\n",
    "bank_abc_links = get_bank_abc_links(question, urlfilter)\n",
    "\n",
    "# Display extracted Bank ABC Links\n",
    "print(\"\\nBank ABC Links:\")\n",
    "print(bank_abc_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
